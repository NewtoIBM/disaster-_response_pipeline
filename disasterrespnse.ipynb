{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disasterrespnse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mE0HUFgbEq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1241ced9-b825-4fc7-827c-44be2fad33d6"
      },
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet'])\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "%matplotlib inline\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sqlalchemy import create_engine\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "#url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating SQL Engine and read a table."
      ],
      "metadata": {
        "id": "VugQw-S30P9A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JxtFypIbO_H"
      },
      "source": [
        "\n",
        "engine = create_engine(\"sqlite:////content/DisasterResponse (1).db\")\n",
        "# load data from database\n",
        "#def load_data():\n",
        "#engine = create_engine('sqlite:///DisasterResponse(1).db')\n",
        "df = pd.read_sql_table('Message', engine)\n",
        "    #engine = create_engine('sqlite:///DisasterResponse.db')\n",
        "    #df = pd.read_sql_table('DisasterResponse', 'sqlite:///DisasterResponse.db') \n",
        "#print(df.head())\n",
        "df = df.drop(['original','index'], axis=1)\n",
        "#X = df.message  \n",
        "\n",
        "#y = df[df.columns[~df.columns.isin(['message','original','genre','id'])]].values\n",
        "    #return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4fIYfEhbSHu"
      },
      "source": [
        "X = df[['message']].values\n",
        "Y = df.iloc[:,4:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6pKiWgVbTHZ"
      },
      "source": [
        "X = [i for i in X for i in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge0OV2JSbYWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce2e73a-2fc8-409b-8958-d12a4251e3f9"
      },
      "source": [
        "Y1 = df.iloc[:,4].values\n",
        "Y1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization sentenses."
      ],
      "metadata": {
        "id": "tj40f-7k0hP1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB7f56XRbZxl"
      },
      "source": [
        "def tokenize(text):\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean_tokens = []\n",
        "    for tok in tokens:\n",
        "        \n",
        "        clean_tok = lemmatizer.lemmatize(tok, pos = 'v').lower().strip()\n",
        "        \n",
        "        clean_tokens.append(clean_tok)\n",
        "    clean_tokens = [w for w in clean_tokens if w not in stopwords.words(\"english\")]\n",
        "\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arlT-tuWbgbd"
      },
      "source": [
        "X1 = word_tokenize(X[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a machine learning pipeline."
      ],
      "metadata": {
        "id": "mgUmDy9E0qDl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v28Pq7QPbhKB"
      },
      "source": [
        "#Build a machine learnng pipeline\n",
        "#for i in model:\n",
        "estimator = RandomForestClassifier()\n",
        "#estimator =  MultinomialNB()\n",
        "pipeline = Pipeline([\n",
        "        ('transformer', Pipeline([\n",
        "            ('vect', CountVectorizer(tokenizer = tokenize)),\n",
        "            ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "         ('clf', MultiOutputClassifier(estimator))\n",
        "        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0wzDJlAbj04"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrx6KFr0bmSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5030bf-7eb0-42d4-8770-c16e3e49eb18"
      },
      "source": [
        "pipeline.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('transformer',\n",
              "                 Pipeline(steps=[('vect',\n",
              "                                  CountVectorizer(tokenizer=<function tokenize at 0x7fe1c8e469e0>)),\n",
              "                                 ('tfidf', TfidfTransformer())])),\n",
              "                ('clf',\n",
              "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWlZZSG2boMO"
      },
      "source": [
        "Y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRKagLeMbqm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057f5440-6a1b-4e94-bd14-5a1475546d1b"
      },
      "source": [
        "categories = ['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food','shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death',  'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
        "for category in categories:\n",
        "# train the model using X_dtm & y\n",
        "    \n",
        "    # compute the testing accuracy\n",
        "    \n",
        "    print(Y_pred[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 represent a classified category. I this case , third category 'offer' was classified to a santense\n",
        "\n",
        "Creating a GridSearch for a pipeline for a better performance to choose best parameters"
      ],
      "metadata": {
        "id": "Bsi6JDkm00Qd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr0Lo2WMbso2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdbdee8-5f2e-416f-dea0-c0db348901c5"
      },
      "source": [
        "parameters = { \n",
        "    'transformer__vect__max_features': [5000, 3000, 1000],\n",
        "    'transformer__vect__ngram_range': ((1,1),(1,2)),\n",
        "    'transformer__tfidf__use_idf': (True, False)\n",
        "            }\n",
        "cv = GridSearchCV(pipeline, param_grid = parameters)\n",
        "cv.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=Pipeline(steps=[('transformer',\n",
              "                                        Pipeline(steps=[('vect',\n",
              "                                                         CountVectorizer(tokenizer=<function tokenize at 0x7fe1c8e469e0>)),\n",
              "                                                        ('tfidf',\n",
              "                                                         TfidfTransformer())])),\n",
              "                                       ('clf',\n",
              "                                        MultiOutputClassifier(estimator=RandomForestClassifier()))]),\n",
              "             param_grid={'transformer__tfidf__use_idf': (True, False),\n",
              "                         'transformer__vect__max_features': [5000, 3000, 1000],\n",
              "                         'transformer__vect__ngram_range': ((1, 1), (1, 2))})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing a pipeline performance after GridSearch."
      ],
      "metadata": {
        "id": "SyCTpvZ705yB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqrpN7CCbxKd"
      },
      "source": [
        "y_pred = cv.predict(X_test)\n",
        "y_pred[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_XKFdIMbzej"
      },
      "source": [
        "\n",
        "categories = ['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food','shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death',  'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
        "for category in categories:\n",
        "    print(y_pred[1])\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}