{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disasterrespnse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mE0HUFgbEq4"
      },
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet'])\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "# import libraries\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sqlalchemy import create_engine\n",
        "import sqlite3\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gor1_grdbO5J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JxtFypIbO_H"
      },
      "source": [
        "from sqlalchemy import create_engine \n",
        "engine = create_engine(\"sqlite:////content/DisasterResponse (1).db\")\n",
        "# load data from database\n",
        "#def load_data():\n",
        "#engine = create_engine('sqlite:///DisasterResponse(1).db')\n",
        "df = pd.read_sql_table('Message', engine)\n",
        "    #engine = create_engine('sqlite:///DisasterResponse.db')\n",
        "    #df = pd.read_sql_table('DisasterResponse', 'sqlite:///DisasterResponse.db') \n",
        "#print(df.head())\n",
        "df = df.drop(['original','index'], axis=1)\n",
        "#X = df.message  \n",
        "\n",
        "#y = df[df.columns[~df.columns.isin(['message','original','genre','id'])]].values\n",
        "    #return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4fIYfEhbSHu"
      },
      "source": [
        "X = df[['message']].values\n",
        "Y = df.iloc[:,4:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6pKiWgVbTHZ"
      },
      "source": [
        "X = [i for i in X for i in i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge0OV2JSbYWW"
      },
      "source": [
        "Y1 = df.iloc[:,4].values\n",
        "Y1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB7f56XRbZxl"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "def tokenize(text):\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean_tokens = []\n",
        "    for tok in tokens:\n",
        "        \n",
        "        clean_tok = lemmatizer.lemmatize(tok, pos = 'v').lower().strip()\n",
        "        \n",
        "        clean_tokens.append(clean_tok)\n",
        "    clean_tokens = [w for w in clean_tokens if w not in stopwords.words(\"english\")]\n",
        "\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arlT-tuWbgbd"
      },
      "source": [
        "X1 = word_tokenize(X[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v28Pq7QPbhKB"
      },
      "source": [
        "#Build a machine learnng pipeline\n",
        "#for i in model:\n",
        "estimator = RandomForestClassifier()\n",
        "#estimator =  MultinomialNB()\n",
        "pipeline = Pipeline([\n",
        "        ('transformer', Pipeline([\n",
        "            ('vect', CountVectorizer(tokenizer = tokenize)),\n",
        "            ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "         ('clf', MultiOutputClassifier(estimator))\n",
        "        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0wzDJlAbj04"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrx6KFr0bmSH"
      },
      "source": [
        "pipeline.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWlZZSG2boMO"
      },
      "source": [
        "Y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRKagLeMbqm7"
      },
      "source": [
        "categories = ['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food','shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death',  'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
        "for category in categories:\n",
        "# train the model using X_dtm & y\n",
        "    \n",
        "    # compute the testing accuracy\n",
        "    \n",
        "    print(Y_pred[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr0Lo2WMbso2"
      },
      "source": [
        "parameters = { \n",
        "    'transformer__vect__max_features': [5000, 3000, 1000],\n",
        "    'transformer__vect__ngram_range': ((1,1),(1,2)),\n",
        "    'transformer__tfidf__use_idf': (True, False)\n",
        "            }\n",
        "cv = GridSearchCV(pipeline, param_grid = parameters)\n",
        "cv.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqrpN7CCbxKd"
      },
      "source": [
        "y_pred = cv.predict(X_test)\n",
        "y_pred[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_XKFdIMbzej"
      },
      "source": [
        "\n",
        "categories = ['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food','shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death',  'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
        "for category in categories:\n",
        "# train the model using X_dtm & y\n",
        "    \n",
        "    # compute the testing accuracy\n",
        "    \n",
        "    print(y_pred[1])\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO9S5PLDcAGL"
      },
      "source": [
        "# Define a pipeline combining a text feature extractor with multi lable classifier\n",
        "NB_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    NB_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    y_pred = NB_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], y_pred)))\n",
        "    emptydf[category]  = NB_pipeline.predict(X_test)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K81d3Gb8cDPI"
      },
      "source": [
        "emptydf = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldQl8lfOcF2S"
      },
      "source": [
        "emptydf.head(37)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}